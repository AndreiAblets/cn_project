{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основні функції та змінні"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Імпортуємо необхідні бібліотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Код попередньої лабки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectRecignition:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        path_main: string -- path for main (train) image\n",
    "        directory_test: string -- directory with all images for testing\n",
    "        directory_save: string -- directory where you want to save all possible savings\n",
    "        n_features: int -- number of features that will be used as a parameter for ORB descriptor (default: 1500)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path_main, directory_test, directory_save, n_features=1500):\n",
    "        self.path_main = path_main\n",
    "        self.directory_test = directory_test\n",
    "        self.directory_save = directory_save\n",
    "        self.n_features = n_features\n",
    "        self.__orb = cv2.ORB_create(nfeatures=n_features)\n",
    "        self.img_main = cv2.imread(path_main, cv2.IMREAD_GRAYSCALE)\n",
    "        self.keypoints_main, self.descriptors_main = self.__orb.detectAndCompute(self.img_main, None)\n",
    "        self.__bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    \n",
    "    \n",
    "    def get_metrics(self, path):\n",
    "        \"\"\"\n",
    "        Return metrics for main and another image\n",
    "        Arguments:\n",
    "            path: string -- path for another image\n",
    "        Returns:\n",
    "            features: int -- number of features\n",
    "            all_matches: int -- number of all matches\n",
    "            true_matches: int -- number of true matches (find by findHomography)\n",
    "            error_all_matches: float -- mean of distances of DMatch objects for all matches\n",
    "            error_true_matches: float -- mean of distances of DMatch objects for true matches\n",
    "            size: tuple -- size of image\n",
    "            time: float -- time of running the function\n",
    "        \"\"\"        \n",
    "        # Initialize an image\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        kp, des = self.__orb.detectAndCompute(img, None)\n",
    "        \n",
    "        # Find features\n",
    "        features = self.n_features\n",
    "        \n",
    "        # Find time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Find all_matches\n",
    "        try:\n",
    "            matches = self.__bf.match(self.descriptors_main, des)\n",
    "        except:\n",
    "            print(\"Something wrong with image\", path)\n",
    "            return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "        matches = sorted(matches, key = lambda x: x.distance)\n",
    "        all_matches = len(matches)\n",
    "        \n",
    "        # Find true_matches\n",
    "        query_pts = np.float32([self.keypoints_main[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        train_pts = np.float32([kp[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        _, mask = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0)\n",
    "        matches_mask = mask.ravel().tolist()\n",
    "        true_matches_list = []\n",
    "        for index, el in enumerate(matches_mask):\n",
    "            if el == 1:\n",
    "                true_matches_list.append(matches[index])\n",
    "        true_matches = len(true_matches_list)\n",
    "        \n",
    "        # Find time\n",
    "        end_time = time.time()\n",
    "        time_ = round(end_time - start_time, 4)\n",
    "        \n",
    "        # Find error_all_matches\n",
    "        if all_matches == 0:\n",
    "            error_all_matches = np.nan\n",
    "        else:\n",
    "            error_all_matches_list = []\n",
    "            for m in matches:\n",
    "                error_all_matches_list.append(m.distance)\n",
    "            error_all_matches = round(np.array(error_all_matches_list).mean(), 4)\n",
    "        \n",
    "        # Find error_true_matches\n",
    "        if true_matches == 0:\n",
    "            error_true_matches = np.nan\n",
    "        else:\n",
    "            error_true_matches_list = []\n",
    "            for m in true_matches_list:\n",
    "                error_true_matches_list.append(m.distance)\n",
    "            error_true_matches = round(np.array(error_true_matches_list).mean(), 4)\n",
    "        \n",
    "        # Find size\n",
    "        size = img.shape\n",
    "        \n",
    "        # Здається, що алгоримт завжди знаходить у районі 10 true_matches, тому будемо вважати 10 еквівалентно 0\n",
    "#         if true_matches < 10:\n",
    "#             true_matches = 0\n",
    "#             error_true_matches = np.nan\n",
    "        \n",
    "        # Return values as tuple\n",
    "        return features, all_matches, true_matches, error_all_matches, error_true_matches, size, time_\n",
    "    \n",
    "        \n",
    "    def get_all_metrics_as_df(self, print_results=False):\n",
    "        \"\"\"\n",
    "        Return all metrics for all images from directory_test as pandas DataFrame\n",
    "        Returns:\n",
    "            df: pandas DataFrame -- a dataframe with all metrics for all images\n",
    "        \"\"\"\n",
    "        all_metrics = []\n",
    "        \n",
    "        for filename in os.listdir(self.directory_test):\n",
    "            path = self.directory_test + '\\\\\\\\' + filename\n",
    "            temp_list = list(self.get_metrics(path))\n",
    "            temp_list.insert(0, filename)\n",
    "            all_metrics.append(temp_list)\n",
    "            \n",
    "        df = pd.DataFrame(all_metrics, columns=['name', 'features', 'all_matches', 'true_matches', \n",
    "                                                'error_all_matches', 'error_true_matches', 'size', 'time'])\n",
    "        return df\n",
    "        \n",
    "    \n",
    "    def save_all_metrics(self, file_name):\n",
    "        \"\"\"\n",
    "        Save all metrics for all images as csv file\n",
    "        Arguments:\n",
    "            file_name: string -- name of the file (without the format)\n",
    "        \"\"\"\n",
    "        df = self.get_all_metrics_as_df()\n",
    "        df.to_csv(self.directory_save + '\\\\\\\\' + file_name + '.csv') \n",
    "    \n",
    "    \n",
    "    def show_features(self, save=False):\n",
    "        \"\"\"\n",
    "        Show features on the main image\n",
    "        Arguments:\n",
    "            save: bool -- save received image or not (default: False)\n",
    "        \"\"\"\n",
    "        img_keys = cv2.drawKeypoints(self.img_main, self.keypoints_main, None)\n",
    "        cv2.namedWindow(\"Image\", cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\"Image\", 600, 600)\n",
    "        cv2.imshow(\"Image\", img_keys)\n",
    "        \n",
    "        if save:\n",
    "            file_name = self.directory_save + '\\\\\\\\' + 'features_for_' + self.path_main.split('\\\\')[-1]\n",
    "            cv2.imwrite(file_name, img_keys)\n",
    "        \n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "    def show_all_matches(self, random=True, path='', save=False):\n",
    "        \"\"\"\n",
    "        Show all matches between main and another image\n",
    "        Arguments:\n",
    "            random: bool -- show all matches for random image if random set to True (default: True)\n",
    "            path: string -- path for another image if random set to False (default: empty string)\n",
    "            save: bool -- save received image or not (default: False)\n",
    "        \"\"\"\n",
    "        if random:\n",
    "            path = self.directory_test + '\\\\\\\\' + np.random.choice(os.listdir(self.directory_test))\n",
    "        else:\n",
    "            if path == '':\n",
    "                return \"Please enter the path or set random to True\"\n",
    "            \n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        kp, des = self.__orb.detectAndCompute(img, None)\n",
    "        \n",
    "        matches = self.__bf.match(self.descriptors_main, des)\n",
    "        matches = sorted(matches, key = lambda x: x.distance)\n",
    "        \n",
    "        matching_result = cv2.drawMatches(self.img_main, self.keypoints_main, img, kp, matches, None)\n",
    "        \n",
    "        cv2.namedWindow(\"Matches\", cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\"Matches\", 1200, 600)\n",
    "        cv2.imshow(\"Matches\", matching_result)\n",
    "        \n",
    "        if save:\n",
    "            file_name = self.directory_save + '\\\\\\\\' +  'all_matches_for_' + path.split('\\\\')[-1]\n",
    "            cv2.imwrite(file_name, matching_result)\n",
    "        \n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    \n",
    "    def show_true_matches(self, random=True, path='', save=False):\n",
    "        \"\"\"\n",
    "        Show true matches between main and another image (finding by findHomography)\n",
    "        Arguments:\n",
    "            random: bool -- show all matches for random image if random set to True (default: True)\n",
    "            path: string -- path for another image if random set to False (default: empty string)\n",
    "            save: bool -- save received image or not (default: False)\n",
    "        \"\"\"\n",
    "        if random:\n",
    "            path = self.directory_test + '\\\\\\\\' + np.random.choice(os.listdir(self.directory_test))\n",
    "        else:\n",
    "            if path == '':\n",
    "                return \"Please enter the path or set random to True\"\n",
    "            \n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        kp, des = self.__orb.detectAndCompute(img, None)\n",
    "        \n",
    "        matches = self.__bf.match(self.descriptors_main, des)\n",
    "        matches = sorted(matches, key = lambda x: x.distance)\n",
    "        \n",
    "        matching_result = cv2.drawMatches(self.img_main, self.keypoints_main, img, kp, matches, None)\n",
    "        \n",
    "        query_pts = np.float32([self.keypoints_main[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        train_pts = np.float32([kp[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        _, mask = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0)\n",
    "        matches_mask = mask.ravel().tolist()\n",
    "        \n",
    "        true_matches = []\n",
    "        for index, el in enumerate(matches_mask):\n",
    "            if el == 1:\n",
    "                true_matches.append(matches[index])\n",
    "                \n",
    "        matching_true_relults = cv2.drawMatches(self.img_main, self.keypoints_main, img, kp, true_matches, None)\n",
    "        \n",
    "        cv2.namedWindow(\"True Matches\", cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\"True Matches\", 1200, 600)\n",
    "        cv2.imshow(\"True Matches\", matching_true_relults)\n",
    "        \n",
    "        if save:\n",
    "            file_name = self.directory_save + '\\\\\\\\' + 'true_matches_for_' + path.split('\\\\')[-1]\n",
    "            cv2.imwrite(file_name, matching_true_relults)\n",
    "        \n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оголошуємо необхідні змінні"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 500\n",
    "directory_test = \"Library\"\n",
    "include_other_photos = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ініціалізуємо X та y для майбутнього класифікатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_and_y(n_features, directory_test, include_other_photos):\n",
    "    \"\"\"\n",
    "    Create X and y for future classifier\n",
    "    Arguments:\n",
    "        n_features: int -- number of features (will be used for defining ORB descriptor)\n",
    "        directory_test: str -- directory with all images for testing\n",
    "        include_other_photos: bool -- spicifies which classifier will be later (binary classifier with only two possible objects\n",
    "                              on photo or ternary classifier with two possible objects on photo and some another type of photos)\n",
    "    Returns:\n",
    "    X: np.array -- matrix X for future classifier\n",
    "    y: np.array -- vector y for future classifier\n",
    "    \"\"\"\n",
    "    X_dict = {}\n",
    "    X = []\n",
    "    y = []\n",
    "    orb = cv2.ORB_create(nfeatures=n_features)\n",
    "    \n",
    "    for filename in os.listdir(directory_test):\n",
    "        path = directory_test + '\\\\\\\\' + filename\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        kp, des = orb.detectAndCompute(img, None)\n",
    "        try:\n",
    "            size = des.shape[0]\n",
    "        except:\n",
    "            continue\n",
    "        if size == 500:\n",
    "            id = int(filename.split('.')[0])\n",
    "            if include_other_photos:\n",
    "                X.append(des)\n",
    "                X_dict[id] = des\n",
    "                if id < 120:\n",
    "                    y.append(2)\n",
    "                elif id > 225:\n",
    "                    y.append(0)\n",
    "                else:\n",
    "                    y.append(1)\n",
    "            else:\n",
    "                if id < 120:\n",
    "                    X.append(des)\n",
    "                    X_dict[id] = des\n",
    "                    y.append(2)\n",
    "                elif id > 225:\n",
    "                    pass\n",
    "                else:\n",
    "                    X.append(des)\n",
    "                    X_dict[id] = des\n",
    "                    y.append(1)\n",
    "                    \n",
    "    X = np.asarray(X)\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return X, y, X_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Допоміжна функція для наочної перевірки правильності роботи класифікатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_photo(y_test, y_pred, X_test, X_dict):\n",
    "    if y_test == 0:\n",
    "        print(\"This is neither car nor ship!\")\n",
    "    elif y_test == 1:\n",
    "        print(\"This is car!\")\n",
    "    else:\n",
    "        print(\"This is ship!\")\n",
    "    if y_pred == 0:\n",
    "        print(\"My classifier thinks this is neither car nor ship!\")\n",
    "    elif y_pred == 1:\n",
    "        print(\"My classifier thinks this is car!\")\n",
    "    else:\n",
    "        print(\"My classifier thinks this is ship!\")\n",
    "    X_test = X_test.reshape(500, 32)\n",
    "    id = [id for id, el in X_dict.items() if (el == X_test).all()][0]\n",
    "    path = directory_test + '\\\\\\\\' + str(id) + '.jpg'\n",
    "    img = cv2.imread(path, 0)\n",
    "    cv2.namedWindow(\"Image\", cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(\"Image\", 600, 600)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Будуємо тернарний класифікатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (221, 16000)\n",
      "y shape: (221,)\n"
     ]
    }
   ],
   "source": [
    "X, y, X_dict = create_X_and_y(n_features, directory_test, include_other_photos)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Розділяємо вибірку на train та test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (165, 16000)\n",
      "y_train shape: (165,)\n",
      "X_test shape: (56, 16000)\n",
      "y_test shape: (56,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Використовуємо Logistic Regression як наш класифікатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Виводимо точність для одного із train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Демонструємо наглядну роботу класифікатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1,\n",
       "       2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 2, 0, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 2, 1, 1, 1, 2, 1,\n",
       "       2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 0, 2, 1, 2, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як ми бачимо, точність класифіктора для даного train_test_split майже 80%, що впринципі досить непогано для такої трейнової вибірки. Вище я вивів очікувані значення y та передбачені. За допомогою функції print_photo можемо повиводити зображення, які наш класифікатор передбачає не вірно.\n",
    "\n",
    "<img src=\"Library\\146.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is car!\n",
      "My classifier thinks this is ship!\n",
      "Library\\\\146.jpg\n"
     ]
    }
   ],
   "source": [
    "print_photo(y_test[2], y_pred[2], X_test[2], X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Library\\179.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is car!\n",
      "My classifier thinks this is ship!\n",
      "Library\\\\179.jpg\n"
     ]
    }
   ],
   "source": [
    "print_photo(y_test[5], y_pred[5], X_test[5], X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Library\\238.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is neither car nor ship!\n",
      "My classifier thinks this is ship!\n",
      "Library\\\\238.jpg\n"
     ]
    }
   ],
   "source": [
    "print_photo(y_test[1], y_pred[1], X_test[1], X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Library\\101.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is ship!\n",
      "My classifier thinks this is car!\n",
      "Library\\\\101.jpg\n"
     ]
    }
   ],
   "source": [
    "print_photo(y_test[-1], y_pred[-1], X_test[-1], X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так, за цими прикладами здається дуже дивним, що класифікатор не вірно розпізнає об'єкти на цих зображеннях, проте самі зображення дійсно дуже різні (на одному із них лише трішки видніється машинка, головний ракурс на замку, а останнє фото кораблика взагалі трішки розмите, а, як ми уже знаємо із попередньої лабки, ORB є дуже чутливим до освітлення й розмитості фото), а тому однозначного висновку щодо того чому так відбувається зробити не можна. На мою думку, це відбувається через замалу трейн вибірку (лише 165 об'єктів). Та навіть попри таку невеличку вибірку класифікатор працює із точністю близько 70% (для різних train_test_split різна точність), що досить круто. Одне можна сказати напевне: наша вибірка містить замалу кількість фото без машинки й корабля, саме через це вона взагалі не виводить жодного спрогнозованого 0 (що відповідає зображенню без машинки й корабля). Тому наступним кроком спробуємо розглянути бінарну класифікацію на вибірці, яка міститиме лише фото або машинки, або корабля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Виводимо confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  4]\n",
      " [ 0 19  5]\n",
      " [ 0  2 25]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Це я вивів confusion matrix, із якої досить добре видно кількість правильно спрогнозованих фото та помилки першого й другого роду. Наприклад, класифікатор правильно знаходить 19 машин на фото та 25 кораблів, проте замість того, щоб не знайти нічого, він 4 рази знаходить кораблі й 1 раз машинку, а замість того, щоб знайти машинку, він 5 разів знаходить корабель і тд."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Виводимо точність по cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49551101072840203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P.S. У майбутньому користуватимемось цією метрикою, як основною при порівнянні класифікаторів"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Будуємо бінарний класифіктор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (191, 16000)\n",
      "y shape: (191,)\n"
     ]
    }
   ],
   "source": [
    "X, y, X_dict = create_X_and_y(n_features, directory_test, include_other_photos=False)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Розділяємо вибірку на train та test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (143, 16000)\n",
      "y_train shape: (143,)\n",
      "X_test shape: (48, 16000)\n",
      "y_test shape: (48,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=8)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Використовуємо Logistic Regression як наш класифікатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Виводимо точність для одного із train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8958333333333334\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Виводимо confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22  2]\n",
      " [ 3 21]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Виводимо точність по cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807280701754386\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Останню точність ми й вважаємо нашою основною метрикою при порівнянні класифікаторів, тому ми бачимо, що вона зросла на 10% у порівнянні із попереднім тернарним класифікатором, тобто в 4 із 5 випадків класифікатор правильно знаходить чи машинка на зображенні, чи корабель, що я вважаю мега-круто, як для такої невеличкої трейн вибірки (лише 143 об'єкти)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Розглядаємо інші класифікатори"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розглянемо ще кілька класифікаторів і подивимося, яку точінсть видають вони для тернарного класифікатору (випадок, коли в трейн вибірці є зображення або із машинкою, або із кораблем, або без нічого). Як і раніше, вважатимемо за основну метрику середній результат точності cross validation (уже знаємо, що для логістичної цей показник становить близько 70%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (221, 16000)\n",
      "y shape: (221,)\n"
     ]
    }
   ],
   "source": [
    "X, y, X_dict = create_X_and_y(n_features, directory_test, include_other_photos=True)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Використовуємо SVM with linear kernel як наш класифікатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7021174477696217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "print(cross_val_score(svm, X, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Використовуємо SVM with polynomial kernel як наш класифікатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7108319217014868\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='poly', gamma='scale')\n",
    "print(cross_val_score(svm, X, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Використовуємо PCA\n",
    "\n",
    "До цього моменту ми розглядали класифікатори, які ідеально підходять для випадку, коли кількість фіч набагато більша за кількість трейн екзамплів (у нас 16000 проти 165). Зараз же зменшимо розмірність нашої матриці X із [221x16000] до [221x100] за допомогою Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Використовуємо SVM with Gaussian kernel with PCA як наш класифікатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6940523244871071\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='rbf', gamma='scale')\n",
    "print(cross_val_score(svm, X_pca, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Використовуємо NN with PCA як наш класифікатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4625917560700169\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(200,))\n",
    "print(cross_val_score(nn, X_pca, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Використовуємо Random Forest with PCA як наш класифікатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6953039713909279\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "print(cross_val_score(rfc, X_pca, y, cv=10, scoring='accuracy').mean())\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = dict(n_estimators=[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000])\n",
    "# grid = GridSearchCV(rfc, param_grid, cv=10, scoring='accuracy', return_train_score=False)\n",
    "# grid.fit(X_pca, y)\n",
    "# print(grid.best_score_)\n",
    "# print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Використовуємо Decision Trees with PCA як наш класифікатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655561829474873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "print(cross_val_score(dtc, X_pca, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
